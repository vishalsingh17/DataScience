{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T00:15:22.186990Z",
     "start_time": "2021-07-12T00:15:19.336924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-11 19:15:22,146 [16099] WARNING  py.warnings: /home/user/Data-science/venv/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T00:15:36.969264Z",
     "start_time": "2021-07-12T00:15:24.793624Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1jI1cmxqnwsmC-vbl8dNY6b4aNBtBbKy3\n",
      "To: /home/user/Data-science/visualization/scattertext/Twitter.zip\n",
      "120MB [00:10, 11.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Twitter.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id=1jI1cmxqnwsmC-vbl8dNY6b4aNBtBbKy3'\n",
    "output = 'Twitter.zip'\n",
    "gdown.download(url, output, quiet=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T00:15:48.075943Z",
     "start_time": "2021-07-12T00:15:45.098389Z"
    }
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:40:42.134677Z",
     "start_time": "2021-01-01T03:40:42.117150Z"
    }
   },
   "outputs": [],
   "source": [
    "path_train = 'Data/train/en'\n",
    "path_test = 'Data/test/en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:40:42.257445Z",
     "start_time": "2021-01-01T03:40:42.140228Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_train = [f for f in listdir(path_train) if isfile(join(path_train, f))]\n",
    "tweets_test = [f for f in listdir(path_test) if isfile(join(path_test, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:40:57.266725Z",
     "start_time": "2021-01-01T03:40:57.227569Z"
    }
   },
   "outputs": [],
   "source": [
    "for r in ET.parse(join(path_train,tweets_train[6])).getroot()[0]:\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:41:03.248126Z",
     "start_time": "2021-01-01T03:41:03.236578Z"
    }
   },
   "outputs": [],
   "source": [
    "#ET.parse converts the file into the tree\n",
    "def convert_texts(path_train, files_train):\n",
    "    \n",
    "    doc = []\n",
    "    for i in range(len(files_train)):\n",
    "        #Append the tweets to the corresponding document\n",
    "        try:\n",
    "            doc1 =[r.text for r in ET.parse(join(path_train,files_train[i])).getroot()[0]]\n",
    "            doc.append(' '.join(t for t in doc1))\n",
    "        except:\n",
    "            pass \n",
    "    return doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:41:44.110489Z",
     "start_time": "2021-01-01T03:41:40.142742Z"
    }
   },
   "outputs": [],
   "source": [
    "# list of train and test tweets\n",
    "t_train = convert_texts(path_train, tweets_train)\n",
    "t_test  = convert_texts(path_test, tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:41:46.612088Z",
     "start_time": "2021-01-01T03:41:46.591170Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels(path, files):\n",
    "    dic={}\n",
    "    task=[[],[]]\n",
    "    for e in open(join(path,'truth.txt')).read().split('\\n'):\n",
    "        d=e.split(':::')\n",
    "        if(len(d)==3):\n",
    "            dic.update({d[0]: d[1:]})\n",
    "            task[0].append(d[1])\n",
    "            task[1].append(d[2])\n",
    "    task=[sorted(list(set(t))) for t in task]\n",
    "    \n",
    "    d_task = [{item:i for i, item in enumerate(t)} for t in task]\n",
    "    labels=[[],[]]\n",
    "\n",
    "    for e in files:\n",
    "        try:\n",
    "            ID=e[:-4]\n",
    "            labels[0].append(d_task[0][dic[ID][0]])\n",
    "            labels[1].append(d_task[1][dic[ID][1]])\n",
    "        except:\n",
    "            pass\n",
    "    print(d_task)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:04.579248Z",
     "start_time": "2021-01-01T03:42:04.505843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'female': 0, 'male': 1}, {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}]\n",
      "[{'female': 0, 'male': 1}, {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3600)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train = get_labels(path_train, tweets_train)\n",
    "labels_test  = get_labels(path_test , tweets_test)\n",
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:06.695511Z",
     "start_time": "2021-01-01T03:42:06.687949Z"
    }
   },
   "outputs": [],
   "source": [
    "GENDER_MAP = {0:'female', 1: 'male'}\n",
    "COUNTRY_MAP = {0: 'australia', 1: 'canada', 2: 'great britain', 3: 'ireland', 4: 'new zealand', 5: 'united states'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:10.454165Z",
     "start_time": "2021-01-01T03:42:10.438420Z"
    }
   },
   "outputs": [],
   "source": [
    "replacement_patterns = [\n",
    "    #match url (i.e: https://t.co/5tF5G9VKtq)\n",
    "    (r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ''),\n",
    "\n",
    "    #match user (i.e: @cerpintor )\n",
    "    (r'@\\w+', ''),\n",
    "\n",
    "    #match hashtag (i.e: #WomensMarchOnWashington)\n",
    "    (r'#\\w+', ''),\n",
    "\n",
    "    #Replace \"&...\" with ''\n",
    "    (r'&\\w+', '')\n",
    "]\n",
    "\n",
    "class RegexReplacer(object):\n",
    "    def __init__(self, patterns = replacement_patterns):\n",
    "        self.patterns = [(re.compile(regrex),repl) for (regrex, repl) in\n",
    "                        patterns]\n",
    "    \n",
    "    #Replace the words that match the patterns with replacement words\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:10.471017Z",
     "start_time": "2021-01-01T03:42:10.460338Z"
    }
   },
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:10.486447Z",
     "start_time": "2021-01-01T03:42:10.476563Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    \n",
    "    doc = nlp(\" \".join(text)) \n",
    "    return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:12.443042Z",
     "start_time": "2021-01-01T03:42:10.491506Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "tknz = TweetTokenizer()\n",
    "replacer = RegexReplacer()\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
    "\n",
    "# punctuation\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:42:12.459848Z",
     "start_time": "2021-01-01T03:42:12.447473Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(doc):\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        \n",
    "        doc[i] = give_emoji_free_text(doc[i])\n",
    "        \n",
    "        #Tokenize with replacement\n",
    "        doc[i] = tknz.tokenize(replacer.replace(doc[i]))\n",
    "        \n",
    "        #Filter stopwords, punctuations, and lowercase\n",
    "        doc[i] = [w.lower() for w in doc[i] if w not in punc and w not in ALL_STOP_WORDS]\n",
    "        \n",
    "        # Lemmatize\n",
    "        doc[i] = lemmatization(doc[i], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:44.854063Z",
     "start_time": "2021-01-01T03:42:12.466441Z"
    }
   },
   "outputs": [],
   "source": [
    "t_train_processed = normalize(t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:44.864393Z",
     "start_time": "2021-01-01T03:49:44.855174Z"
    }
   },
   "outputs": [],
   "source": [
    "t_train_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:45.006921Z",
     "start_time": "2021-01-01T03:49:44.866060Z"
    }
   },
   "outputs": [],
   "source": [
    "t_train_processed = np.array(t_train_processed)\n",
    "t_train_processed = np.expand_dims(t_train_processed, 1)\n",
    "\n",
    "combine_arr = np.concatenate((t_train_processed, labels_train.T), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:45.068681Z",
     "start_time": "2021-01-01T03:49:45.059700Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combine_arr, columns=['tweets', 'gender', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:45.116449Z",
     "start_time": "2021-01-01T03:49:45.107669Z"
    }
   },
   "outputs": [],
   "source": [
    "df.gender = df.gender.map(GENDER_MAP)\n",
    "df.country = df.country.map(COUNTRY_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:45.139189Z",
     "start_time": "2021-01-01T03:49:45.117567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fucking, terrify, nemesis, record, bad, enemy...</td>\n",
       "      <td>male</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[poetry, shelf, summer, season, poet, pick, po...</td>\n",
       "      <td>female</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[say, truth, attest, cloud, sky, joke, how, ok...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[seem, right, pretty, accurate, new, breakfast...</td>\n",
       "      <td>female</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[s, word, touch, early, leave, night, morning,...</td>\n",
       "      <td>male</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>[love, xma, beautiful, spot, swim, fancy, glas...</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>[late, genomic, news, thank, late, genomic, ne...</td>\n",
       "      <td>male</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>[think, well, year, early, time, tune, et, cat...</td>\n",
       "      <td>female</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>[find, cojone, sort, keep, diggin, racist, rea...</td>\n",
       "      <td>female</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>[life, bare, ensure, understanding, safety, st...</td>\n",
       "      <td>male</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets  gender        country\n",
       "0     [fucking, terrify, nemesis, record, bad, enemy...    male         canada\n",
       "1     [poetry, shelf, summer, season, poet, pick, po...  female    new zealand\n",
       "2     [say, truth, attest, cloud, sky, joke, how, ok...  female         canada\n",
       "3     [seem, right, pretty, accurate, new, breakfast...  female         canada\n",
       "4     [s, word, touch, early, leave, night, morning,...    male        ireland\n",
       "...                                                 ...     ...            ...\n",
       "3595  [love, xma, beautiful, spot, swim, fancy, glas...  female      australia\n",
       "3596  [late, genomic, news, thank, late, genomic, ne...    male      australia\n",
       "3597  [think, well, year, early, time, tune, et, cat...  female  united states\n",
       "3598  [find, cojone, sort, keep, diggin, racist, rea...  female  united states\n",
       "3599  [life, bare, ensure, understanding, safety, st...    male      australia\n",
       "\n",
       "[3600 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-01T03:49:45.522434Z",
     "start_time": "2021-01-01T03:49:45.140720Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('Data/processed_train_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1626048904600,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
